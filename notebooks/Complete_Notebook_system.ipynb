{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Stuffs\n",
    "import os, sys, time, datetime, collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow Stuffs\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook, tqdm\n",
    "from tqdm import tnrange as trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##/////////| OPTIONS |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\# \n",
    "num_gpus = 4\n",
    "replay_buffer = deque()\n",
    "time_step = 0\n",
    "epsilon = .09\n",
    "gamma = .09\n",
    "replay_size = 500\n",
    "state_dim = 40\n",
    "action_dim = 3\n",
    "stats.g_step = 0\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset.\n",
    "# dataset = np.load('testset.npy')\n",
    "# dataset = np.load('../Megaset.alphagriffin.npy')\n",
    "input_shape = dataset.shape\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset[0]), dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_input = tf.placeholder('float', [None, 60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable( 0, name='global_step', trainable=False )\n",
    "learning_rate = tf.train.exponential_decay( .095,\n",
    "                                            global_step,\n",
    "                                            .000005,\n",
    "                                            0.87,\n",
    "                                            staircase=True,\n",
    "                                            name=\"Learn_decay\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1\n",
    "weight_initializer = tf.contrib.layers.xavier_initializer(uniform=True, seed=None, dtype=tf.float32)\n",
    "# weight_initializer = tf.variance_scaling_initializer(mode=\"fan_avg\", distribution=\"uniform\", scale=sigma)\n",
    "bias_initializer = tf.zeros_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape, stddev=0.1):\n",
    "    initial = tf.truncated_normal(shape, stddev=stddev)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "for i in trange(num_gpus):\n",
    "    with tf.device('/gpu:{}'.format(i)):\n",
    "        W0 = weight_variable([120, 1024])\n",
    "        B0 = bias_variable([1024])\n",
    "        hidden_1 = tf.nn.relu(tf.add(tf.matmul(state_input, W0), B0))\n",
    "        c.append(hidden_1)\n",
    "Q_value = tf.add_n(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output variables\n",
    "action_input = tf.placeholder('float', [None, 3])\n",
    "y_input = tf.placeholder('float', [None])\n",
    "\n",
    "# Training Method\n",
    "Q_action = tf.reduce_sum(tf.multiply(Q_value, action_input), reduction_indices=1)\n",
    "cost = tf.reduce_mean(tf.square(y_input - Q_action))\n",
    "optimizer = tf.train.AdamOptimizer(lr).minimize(\n",
    "    cost, global_step=global_step, colocate_gradients_with_ops=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def egreedy_action(state):\n",
    "    Q_value = self.Q_value.eval(feed_dict={\n",
    "        state_input: [state]\n",
    "    })\n",
    "    Q_value = Q_value[0]\n",
    "\n",
    "    if epsilon <= 0.1:\n",
    "        epsilon_rate = 1\n",
    "    else:\n",
    "        epsilon_rate = 0.95\n",
    "    if time_step > 200:\n",
    "        self.epsilon = epsilon_rate * epsilon\n",
    "\n",
    "    if random.random() <= epsilon:\n",
    "        return random.randint(0, 2)\n",
    "    else:\n",
    "        return np.argmax(Q_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action(state):\n",
    "    Q_value = Q_value(feed_dict={\n",
    "        state_input: [state]\n",
    "    })\n",
    "    Q_value = Q_value[0]\n",
    "    return np.argmax(Q_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( state, action, reward, state_, done):\n",
    "        time_step += 1\n",
    "        one_hot_action = np.zeros(3)\n",
    "        one_hot_action[action] = 1\n",
    "        replay_buffer.append((state, one_hot_action, reward, state_, done))\n",
    "        if len(replay_buffer) > replay_size:  # this is a rolling window\n",
    "            replay_buffer.popleft()\n",
    "        if len(replay_buffer) > 100:  # after 100 step ,pre  train\n",
    "            training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(self):\n",
    "        # get random  sample from replay buffer\n",
    "        minibatch = random.sample(replay_buffer, batch_size)\n",
    "        state_batch = [data[0] for data in minibatch]\n",
    "        action_batch = [data[1] for data in minibatch]\n",
    "        reward_batch = [data[2] for data in minibatch]\n",
    "        next_state_batch = [data[3] for data in minibatch]\n",
    "\n",
    "        # calcuate Q\n",
    "        Y_batch = []\n",
    "        next_Q = self.Q_value.eval(feed_dict={\n",
    "            state_input: next_state_batch\n",
    "        })\n",
    "\n",
    "        # Build a batch\n",
    "        for i in range(batch_size):\n",
    "            done = minibatch[i][4]\n",
    "        if done:\n",
    "            Y_batch.append(reward_batch[i])\n",
    "        else:\n",
    "            Y_batch.append(reward_batch[i] + gamma * np.max(next_Q[i]))\n",
    "\n",
    "        # Train on that built Batch. Boom.\n",
    "        \"\"\" EXAMPLE\n",
    "        self.optimizer.run(feed_dict ={\n",
    "        self.y_input:Y_bach,\n",
    "        self.action_input:action_bach,\n",
    "        self.state_input:state_bach\n",
    "        })\n",
    "            \"\"\"\n",
    "        FEED = {\n",
    "            y_input: Y_batch,\n",
    "            action_input: action_batch,\n",
    "            state_input: state_batch\n",
    "        }\n",
    "\n",
    "\n",
    "        # //////| Training Method |\\\\\\\\\\\\\\#\n",
    "        _, stats.cost, stats.g_step = session.run([\n",
    "                optimizer,\n",
    "                cost,\n",
    "                global_step],\n",
    "                feed_dict=FEED\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ss\n",
    "session = tf.InteractiveSession(config=tf.ConfigProto(\n",
    "                allow_soft_placement=True,\n",
    "                ))\n",
    "session.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trange(5):\n",
    "    for j in range(5):\n",
    "        iter_data = data[\n",
    "            iter_step * 240: iter_step * 240 + 240\n",
    "            ]\n",
    "        env = StockEnv(iter_data)\n",
    "        s = env.reset()\n",
    "        while True:\n",
    "            # first S is data 0 from first dataloop\n",
    "            # s is recycled from bottom of loop\n",
    "            action = Agent.egreedy_action(s)\n",
    "            s_, reward, done = env.gostep(action)\n",
    "            # print (\"Action: {} | 0 = buy, 1 = sell\".format(action))\n",
    "            # print (\"Reward: {:.4f}\".format(reward))\n",
    "            # print (\"Current Price set: {}\".format(s_))\n",
    "            Agent.train(s, action, reward, s_, done)\n",
    "            # print (\"Prediction made...\")\n",
    "            # print(\"Total Cash on hand?: {}\".format(env.cash))\n",
    "            s = s_\n",
    "            if done:\n",
    "                # print(\"done\")\n",
    "                tqdm.write(\"Total Cash on hand?: {:.8f}\".format(env.cash))\n",
    "                tqdm.write(\"Total Global Steps: {}\".format(Agent.stats.g_step))\n",
    "                tqdm.write(\"Total Loss of Model: {}\".format(Agent.stats.cost))\n",
    "                break\n",
    "    Agent.save_or_load()\n",
    "    tqdm.write('Saved @ Global Step: {}, Total Loss: {}'.format(\n",
    "        Agent.stats.g_step, Agent.stats.cost\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {
    "height": "48px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Quick Links",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 846,
   "position": {
    "height": "868px",
    "left": "1540px",
    "right": "20px",
    "top": "57px",
    "width": "330px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
